{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Mounting google drive and importing packages\n",
        "\n"
      ],
      "metadata": {
        "id": "gSYd0jBnh0bs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Rfqx60Gxxcc",
        "outputId": "8e03616c-7166-4794-8375-fcb4d8b8476d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j2zors0y1WGL",
        "outputId": "a2e537f6-be0a-47fd-82dc-fc26ead1997c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.11/dist-packages (17.0.0)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.11/dist-packages (from pyarrow) (1.26.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyarrow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v7QUSvJE1iZj"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from collections import Counter\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import IterableDataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data loading"
      ],
      "metadata": {
        "id": "AjX11yB5u4Dx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XL3gOTRP1kUY"
      },
      "outputs": [],
      "source": [
        "df_train_wikitext_2= pd.read_parquet('/content/drive/MyDrive/002_Sem_2/758O - AI/wikitext-2-raw-v1/train-00000-of-00001.parquet')\n",
        "df_test_wikitext_2 = pd.read_parquet('/content/drive/MyDrive/002_Sem_2/758O - AI/wikitext-2-raw-v1/test-00000-of-00001.parquet')\n",
        "df_valid_wikitext_2 = pd.read_parquet('/content/drive/MyDrive/002_Sem_2/758O - AI/wikitext-2-raw-v1/validation-00000-of-00001.parquet')\n",
        "df_train_wikitext103_1= pd.read_parquet('/content/drive/MyDrive/002_Sem_2/758O - AI/wikitext-103-raw-v1/test-00000-of-00001.parquet')\n",
        "df_train_wikitext103_2= pd.read_parquet('/content/drive/MyDrive/002_Sem_2/758O - AI/wikitext-103-raw-v1/train-00001-of-00002.parquet')\n",
        "df_test_wikitext103 = pd.read_parquet('/content/drive/MyDrive/002_Sem_2/758O - AI/wikitext-103-raw-v1/test-00000-of-00001.parquet')\n",
        "df_valid_wikitext103 = pd.read_parquet('/content/drive/MyDrive/002_Sem_2/758O - AI/wikitext-103-raw-v1/validation-00000-of-00001.parquet')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# List of all DataFrames\n",
        "dataframes = {\n",
        "    \"df_train_wikitext_2\": df_train_wikitext_2,\n",
        "    \"df_test_wikitext_2\": df_test_wikitext_2,\n",
        "    \"df_valid_wikitext_2\": df_valid_wikitext_2,\n",
        "    \"df_train_wikitext103_1\": df_train_wikitext103_1,\n",
        "    \"df_train_wikitext103_2\": df_train_wikitext103_2,\n",
        "    \"df_test_wikitext103\": df_test_wikitext103,\n",
        "    \"df_valid_wikitext103\": df_valid_wikitext103\n",
        "}\n",
        "\n",
        "# Inspect the top 5 rows and column names\n",
        "for name, df in dataframes.items():\n",
        "    print(f\"\\n--- {name} ---\")\n",
        "    print(\"Columns:\", df.columns.tolist())  # Display column names\n",
        "    print(df.head(5))  # Display the first 5 rows\n",
        "    print(\"\\n\" + \"=\"*50)  # Separator for readability\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tkFCifCymltg",
        "outputId": "52c3db0b-96f3-4bed-c25a-877ac07469fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- df_train_wikitext_2 ---\n",
            "Columns: ['text']\n",
            "                                                text\n",
            "0                                                   \n",
            "1                     = Valkyria Chronicles III = \\n\n",
            "2                                                   \n",
            "3   Senj≈ç no Valkyria 3 : Unrecorded Chronicles (...\n",
            "4   The game began development in 2010 , carrying...\n",
            "\n",
            "==================================================\n",
            "\n",
            "--- df_test_wikitext_2 ---\n",
            "Columns: ['text']\n",
            "                                                text\n",
            "0                                                   \n",
            "1                              = Robert Boulter = \\n\n",
            "2                                                   \n",
            "3   Robert Boulter is an English film , televisio...\n",
            "4   In 2006 , Boulter starred alongside Whishaw i...\n",
            "\n",
            "==================================================\n",
            "\n",
            "--- df_valid_wikitext_2 ---\n",
            "Columns: ['text']\n",
            "                                                text\n",
            "0                                                   \n",
            "1                            = Homarus gammarus = \\n\n",
            "2                                                   \n",
            "3   Homarus gammarus , known as the European lobs...\n",
            "4                                                   \n",
            "\n",
            "==================================================\n",
            "\n",
            "--- df_train_wikitext103_1 ---\n",
            "Columns: ['text']\n",
            "                                                text\n",
            "0                                                   \n",
            "1                              = Robert Boulter = \\n\n",
            "2                                                   \n",
            "3   Robert Boulter is an English film , televisio...\n",
            "4   In 2006 , Boulter starred alongside Whishaw i...\n",
            "\n",
            "==================================================\n",
            "\n",
            "--- df_train_wikitext103_2 ---\n",
            "Columns: ['text']\n",
            "                                  text\n",
            "0   ( * ) Denotes co @-@ producer . \\n\n",
            "1                                     \n",
            "2     = = Credits and personnel = = \\n\n",
            "3                                     \n",
            "4   Credits adapted from Allmusic . \\n\n",
            "\n",
            "==================================================\n",
            "\n",
            "--- df_test_wikitext103 ---\n",
            "Columns: ['text']\n",
            "                                                text\n",
            "0                                                   \n",
            "1                              = Robert Boulter = \\n\n",
            "2                                                   \n",
            "3   Robert Boulter is an English film , televisio...\n",
            "4   In 2006 , Boulter starred alongside Whishaw i...\n",
            "\n",
            "==================================================\n",
            "\n",
            "--- df_valid_wikitext103 ---\n",
            "Columns: ['text']\n",
            "                                                text\n",
            "0                                                   \n",
            "1                            = Homarus gammarus = \\n\n",
            "2                                                   \n",
            "3   Homarus gammarus , known as the European lobs...\n",
            "4                                                   \n",
            "\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train_1 = pd.concat([df_train_wikitext_2, df_train_wikitext103_1, df_train_wikitext103_2], ignore_index=True) # Merging training data\n",
        "\n",
        "df_test = pd.concat([df_test_wikitext_2, df_test_wikitext103], ignore_index=True) # Merging testing data\n",
        "\n",
        "df_valid = pd.concat([df_valid_wikitext_2, df_valid_wikitext103], ignore_index=True) # Merging validation data\n",
        "\n",
        "# Checking shape of dataframaes after merging to ensure consistency\n",
        "print(\"Train Data Shape:\", df_train_1.shape)\n",
        "print(\"Test Data Shape:\", df_test.shape)\n",
        "print(\"Validation Data Shape:\", df_valid.shape)\n",
        "\n",
        "# Preview first few rows of each combined dataset\n",
        "print(\"\\n--- Train Data Sample ---\\n\", df_train_1.head())\n",
        "print(\"\\n--- Test Data Sample ---\\n\", df_test.head())\n",
        "print(\"\\n--- Validation Data Sample ---\\n\", df_valid.head())\n"
      ],
      "metadata": {
        "id": "Je0QrUY2Z3pP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5a393aa-5087-4f93-f75d-dbb8cc61b52c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Data Shape: (941751, 1)\n",
            "Test Data Shape: (8716, 1)\n",
            "Validation Data Shape: (7520, 1)\n",
            "\n",
            "--- Train Data Sample ---\n",
            "                                                 text\n",
            "0                                                   \n",
            "1                     = Valkyria Chronicles III = \\n\n",
            "2                                                   \n",
            "3   Senj≈ç no Valkyria 3 : Unrecorded Chronicles (...\n",
            "4   The game began development in 2010 , carrying...\n",
            "\n",
            "--- Test Data Sample ---\n",
            "                                                 text\n",
            "0                                                   \n",
            "1                              = Robert Boulter = \\n\n",
            "2                                                   \n",
            "3   Robert Boulter is an English film , televisio...\n",
            "4   In 2006 , Boulter starred alongside Whishaw i...\n",
            "\n",
            "--- Validation Data Sample ---\n",
            "                                                 text\n",
            "0                                                   \n",
            "1                            = Homarus gammarus = \\n\n",
            "2                                                   \n",
            "3   Homarus gammarus , known as the European lobs...\n",
            "4                                                   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = df_train_1.sample(n=200000, random_state=42).reset_index(drop=True)\n"
      ],
      "metadata": {
        "id": "etTYeqFo_z9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Cleaning"
      ],
      "metadata": {
        "id": "mwsl5PvjuxPs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean the data by defining a fucntion for it which does the following :-\n",
        "# Convert to lowercas.\n",
        "# Remove all characters that are not alphabets or spaces\n",
        "# Replace multiple spaces with a single space\n",
        "# Strip leading and trailing spaces\n",
        "def text_clean(text):\n",
        "\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = text.strip()\n",
        "    return text\n",
        "\n",
        "# Apply the cleaing to the 'text' column in each DataFrame\n",
        "df_train['clean_text'] = df_train['text'].apply(text_clean)\n",
        "df_test['clean_text'] = df_test['text'].apply(text_clean)\n",
        "df_valid['clean_text'] = df_valid['text'].apply(text_clean)\n",
        "\n",
        "# Preview the cleaned text for verification\n",
        "print(\"Cleaned Train Data Sample:\")\n",
        "print(df_train[['text', 'clean_text']].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q0csda8CsMjx",
        "outputId": "cfeb9d1d-9edd-4a51-d0b0-696a747bf9c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned Train Data Sample:\n",
            "                                                text  \\\n",
            "0                           = = = Stalemate = = = \\n   \n",
            "1   On 6 April 1320 the Scottish Parliament met a...   \n",
            "2   Following his return from Algeria and his com...   \n",
            "3   = = = Roman occupation to Medieval period = =...   \n",
            "4                                                      \n",
            "\n",
            "                                          clean_text  \n",
            "0                                          stalemate  \n",
            "1  on april the scottish parliament met at arbroa...  \n",
            "2  following his return from algeria and his comp...  \n",
            "3                roman occupation to medieval period  \n",
            "4                                                     \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dropping the text column with unclean data and only keeping the clean text column\n",
        "df_train.drop('text', axis=1, inplace=True)\n",
        "df_test.drop('text', axis=1, inplace=True)\n",
        "df_valid.drop('text', axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "Cbc4tF-eSgpx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# removeing all the blank cells from the dataframes\n",
        "df_train = df_train[df_train['clean_text'].str.strip() != '']\n",
        "df_test = df_test[df_test['clean_text'].str.strip() != '']\n",
        "df_valid = df_valid[df_valid['clean_text'].str.strip() != '']"
      ],
      "metadata": {
        "id": "cCe-aXZQTdVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.head(100) # for visual confirmation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "-zphSr0btlJK",
        "outputId": "10cd753c-817a-4ab6-f5b9-9bf263dfca04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                            clean_text\n",
              "0                                            stalemate\n",
              "1    on april the scottish parliament met at arbroa...\n",
              "2    following his return from algeria and his comp...\n",
              "3                  roman occupation to medieval period\n",
              "6    nebraska was widely lauded by television comme...\n",
              "..                                                 ...\n",
              "147  on june mccormack returned to duty with the wa...\n",
              "149                                   falkland islands\n",
              "150           my head cool bedded in the flowery grass\n",
              "152  in line with christian interpretation of the d...\n",
              "153                   hitler and the sturmabteilung sa\n",
              "\n",
              "[100 rows x 1 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b728c2ad-3a74-4e06-8b49-bebc6269f2f2\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>clean_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>stalemate</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>on april the scottish parliament met at arbroa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>following his return from algeria and his comp...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>roman occupation to medieval period</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>nebraska was widely lauded by television comme...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>147</th>\n",
              "      <td>on june mccormack returned to duty with the wa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149</th>\n",
              "      <td>falkland islands</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>150</th>\n",
              "      <td>my head cool bedded in the flowery grass</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>152</th>\n",
              "      <td>in line with christian interpretation of the d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>153</th>\n",
              "      <td>hitler and the sturmabteilung sa</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows √ó 1 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b728c2ad-3a74-4e06-8b49-bebc6269f2f2')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b728c2ad-3a74-4e06-8b49-bebc6269f2f2 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b728c2ad-3a74-4e06-8b49-bebc6269f2f2');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-cde97462-4a6f-4474-a5d1-bb8a83c7b510\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-cde97462-4a6f-4474-a5d1-bb8a83c7b510')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-cde97462-4a6f-4474-a5d1-bb8a83c7b510 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_train"
            }
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To take a look at the shape pf the model\n",
        "print(\"Train Data Shape:\", df_train.shape)\n",
        "print(\"Test Data Shape:\", df_test.shape)\n",
        "print(\"Validation Data Shape:\", df_valid.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZhPWLheWSY30",
        "outputId": "baf356bf-c75d-4447-97e8-74c32aa69ef7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Data Shape: (129029, 1)\n",
            "Test Data Shape: (5778, 1)\n",
            "Validation Data Shape: (4908, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building Vocabulary"
      ],
      "metadata": {
        "id": "6yENMM5Zbu2i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to build the vocabulary and get word frequencies from training data, because this a skip gram model we will direclty have the every single word as a token.\n",
        "def build_vocab_from_train(df, min_freq=1):\n",
        "    counter = Counter()\n",
        "    for text in df['clean_text']:\n",
        "        words = text.split()  # Simple whitespace tokenization\n",
        "        counter.update(words)\n",
        "    # Filter words by minimum frequency if desired, we have kept it 1  this is to reduce the number odf words in the vaocab which could have a lesser contrinutions to the model\n",
        "    filtered_counter = {word: freq for word, freq in counter.items() if freq >= min_freq}\n",
        "    return Counter(filtered_counter)\n",
        "\n",
        "# Now to set our vocab we set the frquency in a descending manner and take the top 11000 wordsas they also cover 90% the dataset.\n",
        "vocab_counter = build_vocab_from_train(df_train, min_freq=1)\n",
        "total_occurrences = sum(vocab_counter.values())\n",
        "sorted_words = sorted(vocab_counter.items(), key=lambda x: x[1], reverse=True)\n",
        "top_k = 11000\n",
        "top_words = sorted_words[:top_k]\n",
        "\n",
        "top_occurrences = sum(freq for word, freq in top_words)\n",
        "coverage = top_occurrences / total_occurrences\n",
        "coverage_percentage = coverage * 100\n",
        "\n",
        "print(f\"Top {top_k} words cover {coverage_percentage:.2f}% of the corpus.\")\n",
        "\n",
        "# Build the vocabulary for the Word2Vec model using only these 11,000 words\n",
        "word2idx = {word: idx for idx, (word, _) in enumerate(top_words)} # word to unique index paring\n",
        "idx2word = {idx: word for word, idx in word2idx.items()}\n",
        "\n",
        "print(\"Vocabulary size for Word2Vec model:\", len(word2idx))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERPQPRrXpyRJ",
        "outputId": "0f5720c3-ea1e-4f67-8c3b-685ba1c12563"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 11000 words cover 90.34% of the corpus.\n",
            "Vocabulary size for Word2Vec model: 11000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom Data Loader"
      ],
      "metadata": {
        "id": "e779gafCkxYn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generator Function, this yields training pairs using a sliding window approach where we decide the window sixe to be of 2.\n",
        "def generate_training_pairs_stream(texts, word2idx, window_size=2):\n",
        "    \"\"\"\n",
        "    Generator function to yield (center, context) training pairs one at a time.\n",
        "\n",
        "    Parameters:\n",
        "        texts (iterable): An iterable of cleaned text strings.\n",
        "        word2idx (dict): Mapping from words to unique indices.\n",
        "        window_size (int): The size of the context window.\n",
        "\n",
        "    Yields:\n",
        "        tuple: (center_word_index, context_word_index)\n",
        "    \"\"\"\n",
        "    for text in texts:\n",
        "        words = text.split()\n",
        "        # Convert words to indices, only keeping words in our vocabulary.\n",
        "        indices = [word2idx[word] for word in words if word in word2idx]\n",
        "        for center_pos, center_word in enumerate(indices):\n",
        "            start = max(0, center_pos - window_size)\n",
        "            end = min(len(indices), center_pos + window_size + 1)\n",
        "            for context_pos in range(start, end):\n",
        "                if context_pos == center_pos:\n",
        "                    continue  # Skip the center word itself.\n",
        "                yield center_word, indices[context_pos]\n",
        "\n",
        "# IterableDataset Subclass: Streams training pairs without loading all into RAM.\n",
        "# Used this approach as I was facing an issue of RAM constraints when  tred to creat\n",
        "class SkipGramIterableDataset(IterableDataset):\n",
        "    def __init__(self, texts, word2idx, window_size=2):\n",
        "        self.texts = texts\n",
        "        self.word2idx = word2idx\n",
        "        self.window_size = window_size\n",
        "\n",
        "    def __iter__(self):\n",
        "        return generate_training_pairs_stream(self.texts, self.word2idx, self.window_size)\n",
        "\n",
        "# Custom Collate Function: Convert a list of tuples into a single tensor. This was later added when I realised the context pair generator yeiled out put as a Tuple and not a tensor.\n",
        "def collate_fn(batch):\n",
        "    return torch.tensor(batch, dtype=torch.long)\n",
        "\n",
        "# DataLoader Setup is such that it creates the dataset and loads mini-batches on the fly.\n",
        "window_size = 2\n",
        "dataset = SkipGramIterableDataset(df_train['clean_text'], word2idx, window_size)\n",
        "\n",
        "# Creates the dataLoader with the custom collate function earlier defind to give a tensor\n",
        "dataloader = DataLoader(dataset, batch_size=1024, collate_fn=collate_fn)\n",
        "\n",
        "# Fetch and print one batch to verify the setup\n",
        "for batch in dataloader:\n",
        "    print(batch)\n",
        "    break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUYOC-I83AF_",
        "outputId": "6abc6117-4375-411f-d3a1-2cd58446e33f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[  8, 184],\n",
            "        [  8,   0],\n",
            "        [184,   8],\n",
            "        ...,\n",
            "        [162, 106],\n",
            "        [162, 283],\n",
            "        [106,  25]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Defining the model"
      ],
      "metadata": {
        "id": "S1VjsVmwk4VY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Word2VecSkipGram(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim): # Initializes the skip-gram model with two embedding layers.\n",
        "\n",
        "        super(Word2VecSkipGram, self).__init__()\n",
        "        self.center_embeddings = nn.Embedding(vocab_size, embed_dim) # converts center word indices into dense vector\n",
        "        self.context_embeddings = nn.Embedding(vocab_size, embed_dim) # converts context word indices into dense vectors.\n",
        "\n",
        "        # Initializes the embeddings uniformly to stablise training\n",
        "        initrange = 0.5 / embed_dim\n",
        "        self.center_embeddings.weight.data.uniform_(-initrange, initrange)\n",
        "        self.context_embeddings.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, center_words, context_words): #Forward pass for the skip-gram model.\n",
        "\n",
        "        center_embed = self.center_embeddings(center_words) # Retrieves the embeddings for the center  words.\n",
        "        context_embed = self.context_embeddings(context_words)  # Retrieves context word embeddings.\n",
        "\n",
        "        # Compute the dot product between the center and context embeddings.\n",
        "        scores = torch.sum(center_embed * context_embed, dim=1)\n",
        "\n",
        "        return scores\n",
        "\n",
        "# Example usage of the model -\n",
        "vocab_size = len(word2idx)\n",
        "embed_dim = 100\n",
        "\n",
        "model = Word2VecSkipGram(vocab_size, embed_dim)\n"
      ],
      "metadata": {
        "id": "w5G0ejb_1p4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the model"
      ],
      "metadata": {
        "id": "xaVx0pqrlALH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "embed_dim = 100              # Dimension of word embeddings\n",
        "vocab_size = len(word2idx)\n",
        "neg_sample_size = 5           # Number of negative samples per positive pair\n",
        "learning_rate = 0.001\n",
        "num_epochs = 1                # Will keep it one as this we check how much time it takes to train a single one\n",
        "\n",
        "# Instantiating the model, optimizer, and loss function\n",
        "model = Word2VecSkipGram(vocab_size, embed_dim)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "loss_function = nn.BCEWithLogitsLoss()  # Using the Binary Cross Entropy with Logits Loss fucntion - since the model outputs a dot product score for each pair. This loss fucntion turns them into probabilities, and then compares them with the target labels (1 for positive pairs, 0 for negative pairs) to compute the loss.\n",
        "\n",
        "model.train()\n",
        "update_count = 0  # Counter for parameter updates to actully see how many time are the parameter getting updated\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0.0\n",
        "    for batch in dataloader:\n",
        "        update_count += 1\n",
        "\n",
        "        # Each batch is of shape (batch_size, 2) containing (center, context) pairs\n",
        "        center_words = batch[:, 0]       # shape: (batch_size,)\n",
        "        pos_context_words = batch[:, 1]    # shape: (batch_size,)\n",
        "        batch_size = center_words.size(0)\n",
        "\n",
        "        # Positive Examples\n",
        "        pos_score = model(center_words, pos_context_words)  # shape: (batch_size,)\n",
        "        pos_labels = torch.ones_like(pos_score)  # Target label = 1 for positive pairs\n",
        "\n",
        "        # Negative Examples\n",
        "        neg_context_words = torch.randint(low=0, high=vocab_size, size=(batch_size, neg_sample_size),\n",
        "                                          device=center_words.device)\n",
        "        center_embed = model.center_embeddings(center_words)  # (batch_size, embed_dim)\n",
        "        neg_context_embed = model.context_embeddings(neg_context_words)  # (batch_size, neg_sample_size, embed_dim)\n",
        "        neg_score = torch.bmm(neg_context_embed, center_embed.unsqueeze(2)).squeeze(2)  # (batch_size, neg_sample_size)\n",
        "        neg_labels = torch.zeros_like(neg_score)  # Target label = 0 for negative pairs\n",
        "\n",
        "        # Loss Computation\n",
        "        loss_pos = loss_function(pos_score, pos_labels)\n",
        "        loss_neg = loss_function(neg_score, neg_labels)\n",
        "        loss = loss_pos + loss_neg.mean()\n",
        "\n",
        "        # Backpropagation & Optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Print loss for the current mini-batch update\n",
        "        print(f\"Update {update_count}: Loss = {loss.item():.4f}\")\n",
        "\n",
        "    avg_loss = total_loss / update_count\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "print(f\"Total parameter updates: {update_count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "95tjwhrC2FHO",
        "outputId": "e8d96764-2675-4b28-d39d-f89ee51ace5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Update 1: Loss = 1.3863\n",
            "Update 2: Loss = 1.3863\n",
            "Update 3: Loss = 1.3863\n",
            "Update 4: Loss = 1.3863\n",
            "Update 5: Loss = 1.3863\n",
            "Update 6: Loss = 1.3862\n",
            "Update 7: Loss = 1.3862\n",
            "Update 8: Loss = 1.3862\n",
            "Update 9: Loss = 1.3861\n",
            "Update 10: Loss = 1.3861\n",
            "Update 11: Loss = 1.3860\n",
            "Update 12: Loss = 1.3860\n",
            "Update 13: Loss = 1.3859\n",
            "Update 14: Loss = 1.3859\n",
            "Update 15: Loss = 1.3854\n",
            "Update 16: Loss = 1.3857\n",
            "Update 17: Loss = 1.3852\n",
            "Update 18: Loss = 1.3851\n",
            "Update 19: Loss = 1.3849\n",
            "Update 20: Loss = 1.3845\n",
            "Update 21: Loss = 1.3848\n",
            "Update 22: Loss = 1.3837\n",
            "Update 23: Loss = 1.3839\n",
            "Update 24: Loss = 1.3829\n",
            "Update 25: Loss = 1.3824\n",
            "Update 26: Loss = 1.3829\n",
            "Update 27: Loss = 1.3825\n",
            "Update 28: Loss = 1.3814\n",
            "Update 29: Loss = 1.3818\n",
            "Update 30: Loss = 1.3818\n",
            "Update 31: Loss = 1.3806\n",
            "Update 32: Loss = 1.3802\n",
            "Update 33: Loss = 1.3787\n",
            "Update 34: Loss = 1.3784\n",
            "Update 35: Loss = 1.3761\n",
            "Update 36: Loss = 1.3742\n",
            "Update 37: Loss = 1.3724\n",
            "Update 38: Loss = 1.3729\n",
            "Update 39: Loss = 1.3737\n",
            "Update 40: Loss = 1.3707\n",
            "Update 41: Loss = 1.3679\n",
            "Update 42: Loss = 1.3646\n",
            "Update 43: Loss = 1.3703\n",
            "Update 44: Loss = 1.3691\n",
            "Update 45: Loss = 1.3672\n",
            "Update 46: Loss = 1.3622\n",
            "Update 47: Loss = 1.3621\n",
            "Update 48: Loss = 1.3646\n",
            "Update 49: Loss = 1.3650\n",
            "Update 50: Loss = 1.3621\n",
            "Update 51: Loss = 1.3583\n",
            "Update 52: Loss = 1.3567\n",
            "Update 53: Loss = 1.3560\n",
            "Update 54: Loss = 1.3567\n",
            "Update 55: Loss = 1.3469\n",
            "Update 56: Loss = 1.3436\n",
            "Update 57: Loss = 1.3383\n",
            "Update 58: Loss = 1.3476\n",
            "Update 59: Loss = 1.3386\n",
            "Update 60: Loss = 1.3390\n",
            "Update 61: Loss = 1.3353\n",
            "Update 62: Loss = 1.3329\n",
            "Update 63: Loss = 1.3338\n",
            "Update 64: Loss = 1.3337\n",
            "Update 65: Loss = 1.3282\n",
            "Update 66: Loss = 1.3360\n",
            "Update 67: Loss = 1.3291\n",
            "Update 68: Loss = 1.3230\n",
            "Update 69: Loss = 1.3211\n",
            "Update 70: Loss = 1.3324\n",
            "Update 71: Loss = 1.3180\n",
            "Update 72: Loss = 1.3143\n",
            "Update 73: Loss = 1.3077\n",
            "Update 74: Loss = 1.3116\n",
            "Update 75: Loss = 1.2858\n",
            "Update 76: Loss = 1.3027\n",
            "Update 77: Loss = 1.3016\n",
            "Update 78: Loss = 1.2895\n",
            "Update 79: Loss = 1.2854\n",
            "Update 80: Loss = 1.2802\n",
            "Update 81: Loss = 1.2931\n",
            "Update 82: Loss = 1.2668\n",
            "Update 83: Loss = 1.2696\n",
            "Update 84: Loss = 1.2811\n",
            "Update 85: Loss = 1.2862\n",
            "Update 86: Loss = 1.2598\n",
            "Update 87: Loss = 1.2714\n",
            "Update 88: Loss = 1.2517\n",
            "Update 89: Loss = 1.2554\n",
            "Update 90: Loss = 1.2642\n",
            "Update 91: Loss = 1.2472\n",
            "Update 92: Loss = 1.2544\n",
            "Update 93: Loss = 1.2689\n",
            "Update 94: Loss = 1.2561\n",
            "Update 95: Loss = 1.2529\n",
            "Update 96: Loss = 1.2239\n",
            "Update 97: Loss = 1.2477\n",
            "Update 98: Loss = 1.2257\n",
            "Update 99: Loss = 1.2458\n",
            "Update 100: Loss = 1.2539\n",
            "Update 101: Loss = 1.2313\n",
            "Update 102: Loss = 1.2249\n",
            "Update 103: Loss = 1.2282\n",
            "Update 104: Loss = 1.2110\n",
            "Update 105: Loss = 1.2199\n",
            "Update 106: Loss = 1.2210\n",
            "Update 107: Loss = 1.1817\n",
            "Update 108: Loss = 1.2090\n",
            "Update 109: Loss = 1.2082\n",
            "Update 110: Loss = 1.2143\n",
            "Update 111: Loss = 1.2140\n",
            "Update 112: Loss = 1.1946\n",
            "Update 113: Loss = 1.2119\n",
            "Update 114: Loss = 1.1989\n",
            "Update 115: Loss = 1.1925\n",
            "Update 116: Loss = 1.1879\n",
            "Update 117: Loss = 1.1828\n",
            "Update 118: Loss = 1.1474\n",
            "Update 119: Loss = 1.1796\n",
            "Update 120: Loss = 1.1601\n",
            "Update 121: Loss = 1.1555\n",
            "Update 122: Loss = 1.1747\n",
            "Update 123: Loss = 1.1593\n",
            "Update 124: Loss = 1.1406\n",
            "Update 125: Loss = 1.1714\n",
            "Update 126: Loss = 1.1739\n",
            "Update 127: Loss = 1.1908\n",
            "Update 128: Loss = 1.1611\n",
            "Update 129: Loss = 1.1761\n",
            "Update 130: Loss = 1.1191\n",
            "Update 131: Loss = 1.1652\n",
            "Update 132: Loss = 1.1788\n",
            "Update 133: Loss = 1.1312\n",
            "Update 134: Loss = 1.1381\n",
            "Update 135: Loss = 1.1584\n",
            "Update 136: Loss = 1.1555\n",
            "Update 137: Loss = 1.1881\n",
            "Update 138: Loss = 1.1044\n",
            "Update 139: Loss = 1.1549\n",
            "Update 140: Loss = 1.1381\n",
            "Update 141: Loss = 1.0906\n",
            "Update 142: Loss = 1.2032\n",
            "Update 143: Loss = 1.1663\n",
            "Update 144: Loss = 1.1399\n",
            "Update 145: Loss = 1.1381\n",
            "Update 146: Loss = 1.1223\n",
            "Update 147: Loss = 1.0983\n",
            "Update 148: Loss = 1.1043\n",
            "Update 149: Loss = 1.1095\n",
            "Update 150: Loss = 1.0928\n",
            "Update 151: Loss = 1.0668\n",
            "Update 152: Loss = 1.0777\n",
            "Update 153: Loss = 1.0942\n",
            "Update 154: Loss = 1.1452\n",
            "Update 155: Loss = 1.1176\n",
            "Update 156: Loss = 1.1506\n",
            "Update 157: Loss = 1.1671\n",
            "Update 158: Loss = 1.1363\n",
            "Update 159: Loss = 1.0931\n",
            "Update 160: Loss = 1.1598\n",
            "Update 161: Loss = 1.0691\n",
            "Update 162: Loss = 1.0555\n",
            "Update 163: Loss = 1.0716\n",
            "Update 164: Loss = 1.0702\n",
            "Update 165: Loss = 1.0979\n",
            "Update 166: Loss = 1.1343\n",
            "Update 167: Loss = 1.1542\n",
            "Update 168: Loss = 1.0398\n",
            "Update 169: Loss = 1.0549\n",
            "Update 170: Loss = 1.1066\n",
            "Update 171: Loss = 1.0271\n",
            "Update 172: Loss = 1.0824\n",
            "Update 173: Loss = 1.1335\n",
            "Update 174: Loss = 1.0284\n",
            "Update 175: Loss = 1.1141\n",
            "Update 176: Loss = 1.0840\n",
            "Update 177: Loss = 1.0974\n",
            "Update 178: Loss = 1.0568\n",
            "Update 179: Loss = 1.1383\n",
            "Update 180: Loss = 1.0544\n",
            "Update 181: Loss = 1.0942\n",
            "Update 182: Loss = 1.0589\n",
            "Update 183: Loss = 1.0735\n",
            "Update 184: Loss = 0.9896\n",
            "Update 185: Loss = 1.0562\n",
            "Update 186: Loss = 1.0412\n",
            "Update 187: Loss = 1.0742\n",
            "Update 188: Loss = 1.0387\n",
            "Update 189: Loss = 0.9994\n",
            "Update 190: Loss = 1.1412\n",
            "Update 191: Loss = 1.1067\n",
            "Update 192: Loss = 0.9604\n",
            "Update 193: Loss = 0.9648\n",
            "Update 194: Loss = 1.0169\n",
            "Update 195: Loss = 1.0732\n",
            "Update 196: Loss = 0.9945\n",
            "Update 197: Loss = 1.0399\n",
            "Update 198: Loss = 1.0704\n",
            "Update 199: Loss = 1.0168\n",
            "Update 200: Loss = 1.0036\n",
            "Update 201: Loss = 1.0880\n",
            "Update 202: Loss = 1.0469\n",
            "Update 203: Loss = 1.0025\n",
            "Update 204: Loss = 1.1168\n",
            "Update 205: Loss = 1.1094\n",
            "Update 206: Loss = 0.9757\n",
            "Update 207: Loss = 1.0203\n",
            "Update 208: Loss = 1.0371\n",
            "Update 209: Loss = 1.0191\n",
            "Update 210: Loss = 1.0709\n",
            "Update 211: Loss = 1.1253\n",
            "Update 212: Loss = 1.0092\n",
            "Update 213: Loss = 1.0796\n",
            "Update 214: Loss = 0.9693\n",
            "Update 215: Loss = 1.0778\n",
            "Update 216: Loss = 0.9901\n",
            "Update 217: Loss = 1.0197\n",
            "Update 218: Loss = 1.0247\n",
            "Update 219: Loss = 1.0325\n",
            "Update 220: Loss = 0.9793\n",
            "Update 221: Loss = 1.0077\n",
            "Update 222: Loss = 1.0675\n",
            "Update 223: Loss = 1.0091\n",
            "Update 224: Loss = 1.0398\n",
            "Update 225: Loss = 1.0878\n",
            "Update 226: Loss = 1.0204\n",
            "Update 227: Loss = 1.0785\n",
            "Update 228: Loss = 1.0736\n",
            "Update 229: Loss = 1.0731\n",
            "Update 230: Loss = 1.0456\n",
            "Update 231: Loss = 0.9668\n",
            "Update 232: Loss = 1.0274\n",
            "Update 233: Loss = 1.0316\n",
            "Update 234: Loss = 1.0680\n",
            "Update 235: Loss = 0.9825\n",
            "Update 236: Loss = 1.0119\n",
            "Update 237: Loss = 0.9854\n",
            "Update 238: Loss = 1.0129\n",
            "Update 239: Loss = 1.0323\n",
            "Update 240: Loss = 1.1045\n",
            "Update 241: Loss = 1.0233\n",
            "Update 242: Loss = 0.9364\n",
            "Update 243: Loss = 1.0828\n",
            "Update 244: Loss = 1.0029\n",
            "Update 245: Loss = 0.9587\n",
            "Update 246: Loss = 1.0274\n",
            "Update 247: Loss = 0.9595\n",
            "Update 248: Loss = 1.0513\n",
            "Update 249: Loss = 1.0484\n",
            "Update 250: Loss = 1.0073\n",
            "Update 251: Loss = 0.9911\n",
            "Update 252: Loss = 0.9696\n",
            "Update 253: Loss = 0.9190\n",
            "Update 254: Loss = 1.0381\n",
            "Update 255: Loss = 1.1355\n",
            "Update 256: Loss = 1.0352\n",
            "Update 257: Loss = 1.0036\n",
            "Update 258: Loss = 1.0675\n",
            "Update 259: Loss = 0.9266\n",
            "Update 260: Loss = 1.0022\n",
            "Update 261: Loss = 0.9695\n",
            "Update 262: Loss = 0.9407\n",
            "Update 263: Loss = 1.0134\n",
            "Update 264: Loss = 1.0912\n",
            "Update 265: Loss = 1.0208\n",
            "Update 266: Loss = 0.8893\n",
            "Update 267: Loss = 0.9666\n",
            "Update 268: Loss = 0.8997\n",
            "Update 269: Loss = 1.0262\n",
            "Update 270: Loss = 1.0548\n",
            "Update 271: Loss = 1.0082\n",
            "Update 272: Loss = 0.9531\n",
            "Update 273: Loss = 1.0322\n",
            "Update 274: Loss = 0.9341\n",
            "Update 275: Loss = 0.9273\n",
            "Update 276: Loss = 1.0250\n",
            "Update 277: Loss = 0.9640\n",
            "Update 278: Loss = 0.9267\n",
            "Update 279: Loss = 0.9733\n",
            "Update 280: Loss = 0.9271\n",
            "Update 281: Loss = 0.9700\n",
            "Update 282: Loss = 0.9311\n",
            "Update 283: Loss = 0.9697\n",
            "Update 284: Loss = 0.9535\n",
            "Update 285: Loss = 0.9583\n",
            "Update 286: Loss = 1.0214\n",
            "Update 287: Loss = 0.9673\n",
            "Update 288: Loss = 1.0175\n",
            "Update 289: Loss = 0.9493\n",
            "Update 290: Loss = 1.0263\n",
            "Update 291: Loss = 1.0009\n",
            "Update 292: Loss = 1.1175\n",
            "Update 293: Loss = 1.0015\n",
            "Update 294: Loss = 1.0286\n",
            "Update 295: Loss = 0.9798\n",
            "Update 296: Loss = 1.0059\n",
            "Update 297: Loss = 0.9391\n",
            "Update 298: Loss = 0.9608\n",
            "Update 299: Loss = 0.9115\n",
            "Update 300: Loss = 0.9532\n",
            "Update 301: Loss = 1.0052\n",
            "Update 302: Loss = 0.9680\n",
            "Update 303: Loss = 0.9929\n",
            "Update 304: Loss = 1.0037\n",
            "Update 305: Loss = 0.9628\n",
            "Update 306: Loss = 0.9553\n",
            "Update 307: Loss = 1.0550\n",
            "Update 308: Loss = 0.9544\n",
            "Update 309: Loss = 0.9552\n",
            "Update 310: Loss = 1.0243\n",
            "Update 311: Loss = 0.9721\n",
            "Update 312: Loss = 0.9770\n",
            "Update 313: Loss = 0.8360\n",
            "Update 314: Loss = 0.9451\n",
            "Update 315: Loss = 0.9543\n",
            "Update 316: Loss = 0.9148\n",
            "Update 317: Loss = 0.8822\n",
            "Update 318: Loss = 0.8693\n",
            "Update 319: Loss = 0.9157\n",
            "Update 320: Loss = 1.0604\n",
            "Update 321: Loss = 0.9535\n",
            "Update 322: Loss = 0.9760\n",
            "Update 323: Loss = 1.0100\n",
            "Update 324: Loss = 0.8568\n",
            "Update 325: Loss = 0.9425\n",
            "Update 326: Loss = 0.9673\n",
            "Update 327: Loss = 0.8806\n",
            "Update 328: Loss = 0.9643\n",
            "Update 329: Loss = 1.0038\n",
            "Update 330: Loss = 0.9336\n",
            "Update 331: Loss = 0.9330\n",
            "Update 332: Loss = 1.0544\n",
            "Update 333: Loss = 0.9152\n",
            "Update 334: Loss = 0.9050\n",
            "Update 335: Loss = 0.9542\n",
            "Update 336: Loss = 1.0050\n",
            "Update 337: Loss = 0.9747\n",
            "Update 338: Loss = 0.8573\n",
            "Update 339: Loss = 0.9142\n",
            "Update 340: Loss = 0.8164\n",
            "Update 341: Loss = 0.8772\n",
            "Update 342: Loss = 0.9391\n",
            "Update 343: Loss = 0.9180\n",
            "Update 344: Loss = 0.8806\n",
            "Update 345: Loss = 0.9813\n",
            "Update 346: Loss = 0.9455\n",
            "Update 347: Loss = 0.9640\n",
            "Update 348: Loss = 0.9478\n",
            "Update 349: Loss = 0.8763\n",
            "Update 350: Loss = 0.9547\n",
            "Update 351: Loss = 1.0207\n",
            "Update 352: Loss = 0.9871\n",
            "Update 353: Loss = 0.9708\n",
            "Update 354: Loss = 0.9087\n",
            "Update 355: Loss = 1.0097\n",
            "Update 356: Loss = 0.9714\n",
            "Update 357: Loss = 0.8651\n",
            "Update 358: Loss = 0.9759\n",
            "Update 359: Loss = 0.8326\n",
            "Update 360: Loss = 0.9917\n",
            "Update 361: Loss = 0.9787\n",
            "Update 362: Loss = 1.0532\n",
            "Update 363: Loss = 0.9367\n",
            "Update 364: Loss = 0.9021\n",
            "Update 365: Loss = 0.9135\n",
            "Update 366: Loss = 1.0108\n",
            "Update 367: Loss = 0.9952\n",
            "Update 368: Loss = 0.9555\n",
            "Update 369: Loss = 0.8715\n",
            "Update 370: Loss = 0.9588\n",
            "Update 371: Loss = 0.8837\n",
            "Update 372: Loss = 0.9137\n",
            "Update 373: Loss = 0.9743\n",
            "Update 374: Loss = 0.8415\n",
            "Update 375: Loss = 0.9392\n",
            "Update 376: Loss = 1.0085\n",
            "Update 377: Loss = 0.9908\n",
            "Update 378: Loss = 0.9002\n",
            "Update 379: Loss = 0.7906\n",
            "Update 380: Loss = 0.9252\n",
            "Update 381: Loss = 0.9261\n",
            "Update 382: Loss = 0.9038\n",
            "Update 383: Loss = 0.9536\n",
            "Update 384: Loss = 0.9798\n",
            "Update 385: Loss = 0.9059\n",
            "Update 386: Loss = 0.8634\n",
            "Update 387: Loss = 0.9385\n",
            "Update 388: Loss = 0.8643\n",
            "Update 389: Loss = 0.8897\n",
            "Update 390: Loss = 0.9700\n",
            "Update 391: Loss = 0.9959\n",
            "Update 392: Loss = 0.9902\n",
            "Update 393: Loss = 0.9030\n",
            "Update 394: Loss = 0.8853\n",
            "Update 395: Loss = 0.8725\n",
            "Update 396: Loss = 0.8790\n",
            "Update 397: Loss = 0.8924\n",
            "Update 398: Loss = 0.7475\n",
            "Update 399: Loss = 0.9535\n",
            "Update 400: Loss = 0.9474\n",
            "Update 401: Loss = 0.8567\n",
            "Update 402: Loss = 0.8621\n",
            "Update 403: Loss = 0.8828\n",
            "Update 404: Loss = 0.9208\n",
            "Update 405: Loss = 0.7829\n",
            "Update 406: Loss = 0.9769\n",
            "Update 407: Loss = 0.9687\n",
            "Update 408: Loss = 1.0317\n",
            "Update 409: Loss = 0.9496\n",
            "Update 410: Loss = 0.9236\n",
            "Update 411: Loss = 1.0708\n",
            "Update 412: Loss = 0.7798\n",
            "Update 413: Loss = 0.9362\n",
            "Update 414: Loss = 1.0253\n",
            "Update 415: Loss = 0.9003\n",
            "Update 416: Loss = 0.9245\n",
            "Update 417: Loss = 0.8742\n",
            "Update 418: Loss = 0.8979\n",
            "Update 419: Loss = 0.9501\n",
            "Update 420: Loss = 0.9343\n",
            "Update 421: Loss = 0.8807\n",
            "Update 422: Loss = 0.9939\n",
            "Update 423: Loss = 0.9277\n",
            "Update 424: Loss = 1.0069\n",
            "Update 425: Loss = 0.9513\n",
            "Update 426: Loss = 0.9269\n",
            "Update 427: Loss = 0.8005\n",
            "Update 428: Loss = 0.8763\n",
            "Update 429: Loss = 0.9265\n",
            "Update 430: Loss = 0.8330\n",
            "Update 431: Loss = 0.9333\n",
            "Update 432: Loss = 0.8654\n",
            "Update 433: Loss = 0.7995\n",
            "Update 434: Loss = 0.9694\n",
            "Update 435: Loss = 0.9586\n",
            "Update 436: Loss = 0.9193\n",
            "Update 437: Loss = 0.9922\n",
            "Update 438: Loss = 0.9235\n",
            "Update 439: Loss = 0.9068\n",
            "Update 440: Loss = 0.9010\n",
            "Update 441: Loss = 0.8571\n",
            "Update 442: Loss = 0.8585\n",
            "Update 443: Loss = 0.8890\n",
            "Update 444: Loss = 0.8614\n",
            "Update 445: Loss = 0.9586\n",
            "Update 446: Loss = 0.9089\n",
            "Update 447: Loss = 0.8368\n",
            "Update 448: Loss = 0.8287\n",
            "Update 449: Loss = 0.9493\n",
            "Update 450: Loss = 0.8739\n",
            "Update 451: Loss = 0.8910\n",
            "Update 452: Loss = 0.9948\n",
            "Update 453: Loss = 0.8752\n",
            "Update 454: Loss = 0.9659\n",
            "Update 455: Loss = 0.9085\n",
            "Update 456: Loss = 0.8599\n",
            "Update 457: Loss = 0.8556\n",
            "Update 458: Loss = 0.8784\n",
            "Update 459: Loss = 0.8106\n",
            "Update 460: Loss = 0.8548\n",
            "Update 461: Loss = 0.8116\n",
            "Update 462: Loss = 0.9623\n",
            "Update 463: Loss = 0.8863\n",
            "Update 464: Loss = 0.9551\n",
            "Update 465: Loss = 1.0286\n",
            "Update 466: Loss = 1.0292\n",
            "Update 467: Loss = 1.0208\n",
            "Update 468: Loss = 0.8825\n",
            "Update 469: Loss = 0.8835\n",
            "Update 470: Loss = 0.8929\n",
            "Update 471: Loss = 0.8716\n",
            "Update 472: Loss = 0.8551\n",
            "Update 473: Loss = 0.9602\n",
            "Update 474: Loss = 0.8127\n",
            "Update 475: Loss = 0.8630\n",
            "Update 476: Loss = 0.8514\n",
            "Update 477: Loss = 0.8759\n",
            "Update 478: Loss = 0.7984\n",
            "Update 479: Loss = 0.8914\n",
            "Update 480: Loss = 0.7723\n",
            "Update 481: Loss = 0.9417\n",
            "Update 482: Loss = 0.9171\n",
            "Update 483: Loss = 0.8941\n",
            "Update 484: Loss = 1.0085\n",
            "Update 485: Loss = 0.9299\n",
            "Update 486: Loss = 0.9809\n",
            "Update 487: Loss = 0.9033\n",
            "Update 488: Loss = 0.8936\n",
            "Update 489: Loss = 0.8251\n",
            "Update 490: Loss = 0.8446\n",
            "Update 491: Loss = 0.8373\n",
            "Update 492: Loss = 0.9219\n",
            "Update 493: Loss = 0.9044\n",
            "Update 494: Loss = 0.7931\n",
            "Update 495: Loss = 0.8445\n",
            "Update 496: Loss = 0.7926\n",
            "Update 497: Loss = 0.9524\n",
            "Update 498: Loss = 1.0291\n",
            "Update 499: Loss = 0.8851\n",
            "Update 500: Loss = 0.8706\n",
            "Update 501: Loss = 0.9880\n",
            "Update 502: Loss = 0.7837\n",
            "Update 503: Loss = 0.8496\n",
            "Update 504: Loss = 0.9335\n",
            "Update 505: Loss = 0.8845\n",
            "Update 506: Loss = 0.8962\n",
            "Update 507: Loss = 0.9300\n",
            "Update 508: Loss = 0.9615\n",
            "Update 509: Loss = 0.8725\n",
            "Update 510: Loss = 0.8092\n",
            "Update 511: Loss = 0.9195\n",
            "Update 512: Loss = 0.8193\n",
            "Update 513: Loss = 0.8198\n",
            "Update 514: Loss = 0.9115\n",
            "Update 515: Loss = 0.9101\n",
            "Update 516: Loss = 0.8619\n",
            "Update 517: Loss = 0.8943\n",
            "Update 518: Loss = 0.8606\n",
            "Update 519: Loss = 0.8410\n",
            "Update 520: Loss = 0.8901\n",
            "Update 521: Loss = 1.0962\n",
            "Update 522: Loss = 0.8089\n",
            "Update 523: Loss = 0.9028\n",
            "Update 524: Loss = 0.8527\n",
            "Update 525: Loss = 0.9399\n",
            "Update 526: Loss = 0.7879\n",
            "Update 527: Loss = 0.9121\n",
            "Update 528: Loss = 0.8369\n",
            "Update 529: Loss = 0.8247\n",
            "Update 530: Loss = 0.8694\n",
            "Update 531: Loss = 0.8025\n",
            "Update 532: Loss = 0.9645\n",
            "Update 533: Loss = 0.8324\n",
            "Update 534: Loss = 0.8838\n",
            "Update 535: Loss = 0.8297\n",
            "Update 536: Loss = 0.8151\n",
            "Update 537: Loss = 0.9099\n",
            "Update 538: Loss = 0.8947\n",
            "Update 539: Loss = 0.9021\n",
            "Update 540: Loss = 0.9804\n",
            "Update 541: Loss = 0.8673\n",
            "Update 542: Loss = 0.9185\n",
            "Update 543: Loss = 0.8653\n",
            "Update 544: Loss = 0.8664\n",
            "Update 545: Loss = 0.8417\n",
            "Update 546: Loss = 0.7963\n",
            "Update 547: Loss = 0.9310\n",
            "Update 548: Loss = 0.8620\n",
            "Update 549: Loss = 0.8997\n",
            "Update 550: Loss = 0.9691\n",
            "Update 551: Loss = 0.8963\n",
            "Update 552: Loss = 0.8764\n",
            "Update 553: Loss = 0.9778\n",
            "Update 554: Loss = 0.8503\n",
            "Update 555: Loss = 0.8597\n",
            "Update 556: Loss = 0.9685\n",
            "Update 557: Loss = 0.9035\n",
            "Update 558: Loss = 0.9362\n",
            "Update 559: Loss = 0.8282\n",
            "Update 560: Loss = 0.8842\n",
            "Update 561: Loss = 0.9029\n",
            "Update 562: Loss = 0.9464\n",
            "Update 563: Loss = 0.8685\n",
            "Update 564: Loss = 0.8601\n",
            "Update 565: Loss = 0.7858\n",
            "Update 566: Loss = 0.7845\n",
            "Update 567: Loss = 0.8914\n",
            "Update 568: Loss = 0.9335\n",
            "Update 569: Loss = 0.9288\n",
            "Update 570: Loss = 0.9342\n",
            "Update 571: Loss = 0.8509\n",
            "Update 572: Loss = 0.9020\n",
            "Update 573: Loss = 0.8109\n",
            "Update 574: Loss = 0.8668\n",
            "Update 575: Loss = 0.9108\n",
            "Update 576: Loss = 0.8467\n",
            "Update 577: Loss = 0.9410\n",
            "Update 578: Loss = 0.8829\n",
            "Update 579: Loss = 0.7565\n",
            "Update 580: Loss = 0.9147\n",
            "Update 581: Loss = 0.9447\n",
            "Update 582: Loss = 0.8711\n",
            "Update 583: Loss = 0.7932\n",
            "Update 584: Loss = 0.8848\n",
            "Update 585: Loss = 0.8642\n",
            "Update 586: Loss = 0.8414\n",
            "Update 587: Loss = 0.8072\n",
            "Update 588: Loss = 0.8473\n",
            "Update 589: Loss = 0.8006\n",
            "Update 590: Loss = 0.8352\n",
            "Update 591: Loss = 0.9085\n",
            "Update 592: Loss = 0.8731\n",
            "Update 593: Loss = 0.8399\n",
            "Update 594: Loss = 0.8520\n",
            "Update 595: Loss = 0.7927\n",
            "Update 596: Loss = 0.7683\n",
            "Update 597: Loss = 0.7920\n",
            "Update 598: Loss = 0.9536\n",
            "Update 599: Loss = 0.8322\n",
            "Update 600: Loss = 0.8881\n",
            "Update 601: Loss = 0.7872\n",
            "Update 602: Loss = 0.8661\n",
            "Update 603: Loss = 0.9084\n",
            "Update 604: Loss = 0.7917\n",
            "Update 605: Loss = 0.9106\n",
            "Update 606: Loss = 0.7900\n",
            "Update 607: Loss = 0.8197\n",
            "Update 608: Loss = 0.8401\n",
            "Update 609: Loss = 0.7729\n",
            "Update 610: Loss = 0.9085\n",
            "Update 611: Loss = 0.8221\n",
            "Update 612: Loss = 0.9010\n",
            "Update 613: Loss = 0.7641\n",
            "Update 614: Loss = 0.8716\n",
            "Update 615: Loss = 0.9507\n",
            "Update 616: Loss = 0.8866\n",
            "Update 617: Loss = 0.9503\n",
            "Update 618: Loss = 0.8945\n",
            "Update 619: Loss = 0.8628\n",
            "Update 620: Loss = 0.9000\n",
            "Update 621: Loss = 0.8780\n",
            "Update 622: Loss = 0.8981\n",
            "Update 623: Loss = 0.9718\n",
            "Update 624: Loss = 0.8783\n",
            "Update 625: Loss = 0.9022\n",
            "Update 626: Loss = 0.7924\n",
            "Update 627: Loss = 0.8219\n",
            "Update 628: Loss = 0.9472\n",
            "Update 629: Loss = 0.9573\n",
            "Update 630: Loss = 0.8785\n",
            "Update 631: Loss = 0.7827\n",
            "Update 632: Loss = 0.8284\n",
            "Update 633: Loss = 0.8730\n",
            "Update 634: Loss = 0.9599\n",
            "Update 635: Loss = 0.8538\n",
            "Update 636: Loss = 0.8620\n",
            "Update 637: Loss = 0.8875\n",
            "Update 638: Loss = 0.8902\n",
            "Update 639: Loss = 0.8622\n",
            "Update 640: Loss = 0.8425\n",
            "Update 641: Loss = 0.8983\n",
            "Update 642: Loss = 0.8163\n",
            "Update 643: Loss = 0.9035\n",
            "Update 644: Loss = 0.8511\n",
            "Update 645: Loss = 0.8080\n",
            "Update 646: Loss = 0.9315\n",
            "Update 647: Loss = 0.9367\n",
            "Update 648: Loss = 0.8856\n",
            "Update 649: Loss = 0.8317\n",
            "Update 650: Loss = 0.9163\n",
            "Update 651: Loss = 0.9149\n",
            "Update 652: Loss = 0.9466\n",
            "Update 653: Loss = 0.8390\n",
            "Update 654: Loss = 0.7774\n",
            "Update 655: Loss = 0.8694\n",
            "Update 656: Loss = 0.8903\n",
            "Update 657: Loss = 0.9151\n",
            "Update 658: Loss = 0.8243\n",
            "Update 659: Loss = 0.8957\n",
            "Update 660: Loss = 0.8540\n",
            "Update 661: Loss = 0.7395\n",
            "Update 662: Loss = 0.9310\n",
            "Update 663: Loss = 0.8503\n",
            "Update 664: Loss = 0.9988\n",
            "Update 665: Loss = 0.8842\n",
            "Update 666: Loss = 0.8786\n",
            "Update 667: Loss = 0.8861\n",
            "Update 668: Loss = 0.8556\n",
            "Update 669: Loss = 0.8551\n",
            "Update 670: Loss = 0.9740\n",
            "Update 671: Loss = 0.9099\n",
            "Update 672: Loss = 0.8676\n",
            "Update 673: Loss = 0.8160\n",
            "Update 674: Loss = 0.9022\n",
            "Update 675: Loss = 0.8791\n",
            "Update 676: Loss = 1.0000\n",
            "Update 677: Loss = 0.8260\n",
            "Update 678: Loss = 0.8603\n",
            "Update 679: Loss = 0.9503\n",
            "Update 680: Loss = 0.7935\n",
            "Update 681: Loss = 0.9619\n",
            "Update 682: Loss = 0.8487\n",
            "Update 683: Loss = 0.8807\n",
            "Update 684: Loss = 0.7641\n",
            "Update 685: Loss = 0.8558\n",
            "Update 686: Loss = 0.8945\n",
            "Update 687: Loss = 0.7543\n",
            "Update 688: Loss = 0.9372\n",
            "Update 689: Loss = 0.8566\n",
            "Update 690: Loss = 1.0056\n",
            "Update 691: Loss = 0.7600\n",
            "Update 692: Loss = 0.8784\n",
            "Update 693: Loss = 0.9258\n",
            "Update 694: Loss = 0.8255\n",
            "Update 695: Loss = 0.8845\n",
            "Update 696: Loss = 0.9038\n",
            "Update 697: Loss = 0.8205\n",
            "Update 698: Loss = 0.8441\n",
            "Update 699: Loss = 0.9350\n",
            "Update 700: Loss = 0.8742\n",
            "Update 701: Loss = 0.8693\n",
            "Update 702: Loss = 0.9689\n",
            "Update 703: Loss = 0.8025\n",
            "Update 704: Loss = 0.7427\n",
            "Update 705: Loss = 0.8548\n",
            "Update 706: Loss = 0.9648\n",
            "Update 707: Loss = 0.8357\n",
            "Update 708: Loss = 0.8552\n",
            "Update 709: Loss = 0.8679\n",
            "Update 710: Loss = 0.9168\n",
            "Update 711: Loss = 0.8995\n",
            "Update 712: Loss = 0.8299\n",
            "Update 713: Loss = 0.8560\n",
            "Update 714: Loss = 0.8330\n",
            "Update 715: Loss = 0.9651\n",
            "Update 716: Loss = 0.8691\n",
            "Update 717: Loss = 0.8054\n",
            "Update 718: Loss = 0.8421\n",
            "Update 719: Loss = 0.9280\n",
            "Update 720: Loss = 0.8614\n",
            "Update 721: Loss = 0.7335\n",
            "Update 722: Loss = 0.8068\n",
            "Update 723: Loss = 0.9357\n",
            "Update 724: Loss = 0.8839\n",
            "Update 725: Loss = 0.7971\n",
            "Update 726: Loss = 0.7852\n",
            "Update 727: Loss = 0.7882\n",
            "Update 728: Loss = 0.8411\n",
            "Update 729: Loss = 0.8541\n",
            "Update 730: Loss = 0.7971\n",
            "Update 731: Loss = 0.8498\n",
            "Update 732: Loss = 0.8332\n",
            "Update 733: Loss = 0.8595\n",
            "Update 734: Loss = 0.8165\n",
            "Update 735: Loss = 0.7745\n",
            "Update 736: Loss = 0.8502\n",
            "Update 737: Loss = 0.8244\n",
            "Update 738: Loss = 0.9358\n",
            "Update 739: Loss = 0.7970\n",
            "Update 740: Loss = 0.9410\n",
            "Update 741: Loss = 0.8177\n",
            "Update 742: Loss = 0.8822\n",
            "Update 743: Loss = 0.7716\n",
            "Update 744: Loss = 0.8458\n",
            "Update 745: Loss = 0.8192\n",
            "Update 746: Loss = 0.8187\n",
            "Update 747: Loss = 0.8488\n",
            "Update 748: Loss = 0.9928\n",
            "Update 749: Loss = 0.8003\n",
            "Update 750: Loss = 0.7701\n",
            "Update 751: Loss = 0.7416\n",
            "Update 752: Loss = 0.7827\n",
            "Update 753: Loss = 0.8727\n",
            "Update 754: Loss = 0.8612\n",
            "Update 755: Loss = 0.8577\n",
            "Update 756: Loss = 0.8796\n",
            "Update 757: Loss = 0.8250\n",
            "Update 758: Loss = 0.7688\n",
            "Update 759: Loss = 0.8537\n",
            "Update 760: Loss = 0.9154\n",
            "Update 761: Loss = 0.7722\n",
            "Update 762: Loss = 0.8227\n",
            "Update 763: Loss = 0.8191\n",
            "Update 764: Loss = 0.8435\n",
            "Update 765: Loss = 0.8732\n",
            "Update 766: Loss = 0.8739\n",
            "Update 767: Loss = 0.8387\n",
            "Update 768: Loss = 0.7690\n",
            "Update 769: Loss = 0.8937\n",
            "Update 770: Loss = 0.8572\n",
            "Update 771: Loss = 0.8024\n",
            "Update 772: Loss = 0.8792\n",
            "Update 773: Loss = 0.9044\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-ff0c1b266bd2>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                             )\n\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"differentiable\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    221\u001b[0m             )\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m             adam(\n\u001b[0m\u001b[1;32m    224\u001b[0m                 \u001b[0mparams_with_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m                 \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mmaybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mdisabled_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_fallback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    782\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_single_tensor_adam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    783\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 784\u001b[0;31m     func(\n\u001b[0m\u001b[1;32m    785\u001b[0m         \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    428\u001b[0m                 \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sqs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction2_sqrt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m                 \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction2_sqrt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"word2vec_model_after_1_training_epoch.pt\") # here we save the model and use the same one train it again for 4 more epochs.\n",
        "print(\"Model saved as 'word2vec_model_after_1_training_epoch.pt'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oG_Q_4sYLOS0",
        "outputId": "04ca6462-ef41-4a83-ee57-14bbba0a1820"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved as 'word2vec_model_after_1_training_epoch.pt'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Hyperparameters for further training\n",
        "embed_dim = 100              # Dimension of word embeddings same as the earlier saved model\n",
        "vocab_size = len(word2idx)   # e.g., 11000 from our vocabulary\n",
        "neg_sample_size = 5          # Number of negative samples per positive pair\n",
        "learning_rate = 0.001\n",
        "num_epochs = 4               # Further train for 4 epochs\n",
        "\n",
        "# Re-instantiate the model and load the saved state\n",
        "model = Word2VecSkipGram(vocab_size, embed_dim)\n",
        "model.load_state_dict(torch.load(\"word2vec_model_after_1_training_epoch.pt\"))\n",
        "model.train()\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "loss_function = nn.BCEWithLogitsLoss()\n",
        "\n",
        "update_count = 0            # Reset or continue counting as desired\n",
        "interval_loss = 0.0\n",
        "interval_counter = 0\n",
        "interval_updates = 10000    # Print average loss every 10,000 updates\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss_epoch = 0.0\n",
        "    batch_count = 0\n",
        "\n",
        "    for batch in dataloader:\n",
        "        update_count += 1\n",
        "        interval_counter += 1\n",
        "        batch_count += 1\n",
        "\n",
        "        center_words = batch[:, 0]\n",
        "        pos_context_words = batch[:, 1]\n",
        "        batch_size = center_words.size(0)\n",
        "\n",
        "        # Positive Examples\n",
        "        pos_score = model(center_words, pos_context_words)\n",
        "        pos_labels = torch.ones_like(pos_score)\n",
        "\n",
        "        # Negative Examples\n",
        "        neg_context_words = torch.randint(low=0, high=vocab_size,\n",
        "                                          size=(batch_size, neg_sample_size),\n",
        "                                          device=center_words.device)\n",
        "        center_embed = model.center_embeddings(center_words)\n",
        "        neg_context_embed = model.context_embeddings(neg_context_words)\n",
        "        neg_score = torch.bmm(neg_context_embed, center_embed.unsqueeze(2)).squeeze(2)\n",
        "        neg_labels = torch.zeros_like(neg_score)\n",
        "\n",
        "        # Loss Computation\n",
        "        loss_pos = loss_function(pos_score, pos_labels)\n",
        "        loss_neg = loss_function(neg_score, neg_labels)\n",
        "        loss = loss_pos + loss_neg.mean()\n",
        "\n",
        "        # Backpropagation & Optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss_epoch += loss.item()\n",
        "        interval_loss += loss.item()\n",
        "\n",
        "        if update_count % interval_updates == 0:\n",
        "            avg_interval_loss = interval_loss / interval_counter\n",
        "            print(f\"After {update_count} updates: Average Loss (last {interval_updates} updates) = {avg_interval_loss:.4f}\")\n",
        "            interval_loss = 0.0\n",
        "            interval_counter = 0\n",
        "\n",
        "    avg_loss_epoch = total_loss_epoch / batch_count\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}: Average Loss = {avg_loss_epoch:.4f}\")\n",
        "\n",
        "print(f\"Total parameter updates in further training: {update_count}\")\n",
        "\n",
        "torch.save(model.state_dict(), \"Word2Vec_final_model.pt\")\n",
        "print(\"Final model saved as 'Word2Vec_final_model.pt'\")"
      ],
      "metadata": {
        "id": "-apJlTK_LpDc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "model.load_state_dict(torch.load(\"word2vec_model_after_1_training_epoch.pt\"))\n",
        "After 10000 updates: Average Loss (last 10000 updates) = 0.8028\n",
        "After 20000 updates: Average Loss (last 10000 updates) = 0.7445\n",
        "After 30000 updates: Average Loss (last 10000 updates) = 0.7199\n",
        "Epoch 1/4: Average Loss = 0.7533\n",
        "After 40000 updates: Average Loss (last 10000 updates) = 0.7082\n",
        "After 50000 updates: Average Loss (last 10000 updates) = 0.6978\n",
        "After 60000 updates: Average Loss (last 10000 updates) = 0.6929\n",
        "Epoch 2/4: Average Loss = 0.6977\n",
        "After 70000 updates: Average Loss (last 10000 updates) = 0.6886\n",
        "After 80000 updates: Average Loss (last 10000 updates) = 0.6839\n",
        "After 90000 updates: Average Loss (last 10000 updates) = 0.6810\n",
        "Epoch 3/4: Average Loss = 0.6830\n",
        "After 100000 updates: Average Loss (last 10000 updates) = 0.6786\n",
        "After 110000 updates: Average Loss (last 10000 updates) = 0.6768\n",
        "After 120000 updates: Average Loss (last 10000 updates) = 0.6745\n",
        "Epoch 4/4: Average Loss = 0.6755\n",
        "Total parameter updates in further training: 127596\n",
        "Final model saved as 'Word2Vec_final_model.pt"
      ],
      "metadata": {
        "id": "rveiY2oAeqCb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation"
      ],
      "metadata": {
        "id": "fmGiKFlTlHuR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def print_top_k_similar_words(input_word, word2idx, idx2word, model, top_k=10):\n",
        "\n",
        "    if input_word not in word2idx:\n",
        "        print(f\"'{input_word}' is not in the vocabulary.\")\n",
        "        return\n",
        "\n",
        "\n",
        "    input_idx = word2idx[input_word]  # This retrives  the index and embedding for the input word\n",
        "    input_embedding = model.center_embeddings.weight[input_idx]\n",
        "    normalized_input = F.normalize(input_embedding, dim=0) # Normalize the input embedding to calcualate cosine similarity computation\n",
        "\n",
        "    all_embeddings = model.center_embeddings.weight  # Get all center embeddings\n",
        "    normalized_embeddings = F.normalize(all_embeddings, dim=1) # Normalize all center embeddings\n",
        "\n",
        "    cosine_similarities = torch.mv(normalized_embeddings, normalized_input)  # Computing cosine similarities\n",
        "\n",
        "    top_similarities, top_indices = torch.topk(cosine_similarities, top_k + 1)  # Getting the top K+1 indices including the inout word\n",
        "\n",
        "    print(f\"Top {top_k} words similar to '{input_word}':\")\n",
        "    count = 0\n",
        "    for idx, similarity in zip(top_indices, top_similarities):\n",
        "        if idx.item() == input_idx:\n",
        "            continue  # Skip the input word itself\n",
        "        similar_word = idx2word[idx.item()]\n",
        "        print(f\"{similar_word}: {similarity.item():.4f}\")\n",
        "        count += 1\n",
        "        if count == top_k:\n",
        "            break\n",
        "\n",
        "# Prompt the user for K and the input word:\n",
        "try:\n",
        "    top_k_input = int(input(\"Enter the number of similar words to display (K): \"))\n",
        "except ValueError:\n",
        "    print(\"Invalid input for K. Please enter an integer.\")\n",
        "    top_k_input = 10  # default value\n",
        "\n",
        "user_word = input(\"Enter the input word: \")\n",
        "\n",
        "print_top_k_similar_words(user_word, word2idx, idx2word, model, top_k=top_k_input) # At the end we call the function using the user's input\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FlIXtQUIYLvQ",
        "outputId": "fb1d623a-029c-4085-90db-0ada65a6b23f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the number of similar words to display (K): 10\n",
            "Enter the input word: cat\n",
            "Top 10 words similar to 'cat':\n",
            "rabbi: 0.4042\n",
            "substances: 0.3567\n",
            "forty: 0.3454\n",
            "queen: 0.3351\n",
            "reducing: 0.3346\n",
            "southampton: 0.3344\n",
            "mw: 0.3338\n",
            "serbia: 0.3168\n",
            "confirmed: 0.3127\n",
            "producers: 0.3084\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "gSYd0jBnh0bs",
        "AjX11yB5u4Dx",
        "mwsl5PvjuxPs",
        "6yENMM5Zbu2i",
        "e779gafCkxYn",
        "S1VjsVmwk4VY",
        "xaVx0pqrlALH",
        "fmGiKFlTlHuR"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}